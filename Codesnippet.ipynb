{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18604fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SathyaPriya.Turaga/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the URL for data-1718022718564.csv (press Enter to skip): \n",
      "\n",
      "Conversion of data-1718022718564.csv is successful\n",
      "\n",
      "\n",
      "Successfully converted csv data to json in 2.33 seconds!\n",
      "--------------------------------------------\n",
      "Check: resources\\output.json to resources\\output1.json for results!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Increase the CSV field size limit\n",
    "csv.field_size_limit(int(1e6))  # Increase to 1 million characters\n",
    "\n",
    "def excel_to_csv(excel_file):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file)\n",
    "        df = df.map(str)\n",
    "        csv_file = excel_file.parent / f\"{excel_file.stem}.csv\"\n",
    "        \n",
    "        try:\n",
    "            df = df.drop(['Unnamed: 0'], axis=1)\n",
    "            df.to_csv(csv_file, index=False)      \n",
    "        except:\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            \n",
    "        if csv_file.exists():\n",
    "            return csv_file, excel_file.suffix\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def csv_to_json(csv_file, url, conversion_ext, first_run_flag):\n",
    "    try:\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            data = []\n",
    "            for row in reader:\n",
    "                json_row = {}\n",
    "                content = ', '.join([f\"{key} : {value}\" for key, value in row.items()])\n",
    "                try:\n",
    "                    raw_title = csv_file.name\n",
    "                except:\n",
    "                    raw_title = str(csv_file).split('\\\\')[-1]\n",
    "                doc_name = raw_title.split(\".\")[0] + conversion_ext\n",
    "                json_row['title'] = raw_title.split(\".\")[0]\n",
    "                json_row['content'] = content\n",
    "                json_row['url'] = url\n",
    "                json_row['doc_name'] = doc_name\n",
    "                \n",
    "                data.append(json_row)\n",
    "        \n",
    "        output_file_path = r\"./resources/output.json\"\n",
    "        file_index = 0\n",
    "        \n",
    "        while True:\n",
    "            if file_index > 0:\n",
    "                output_file_path = f\"./resources/output{file_index}.json\"\n",
    "            \n",
    "            if first_run_flag or not Path(output_file_path).is_file():\n",
    "                existing_data = []\n",
    "            else:\n",
    "                with open(output_file_path, 'r', encoding='utf-8') as f:\n",
    "                    existing_data = json.load(f)\n",
    "            \n",
    "            new_data = existing_data + data\n",
    "            json_data = json.dumps(new_data, indent=4)\n",
    "            \n",
    "            if len(json_data.encode('utf-8')) <= 15 * 1024 * 1024:  # 15 MB in bytes\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(json_data)\n",
    "                break\n",
    "            else:\n",
    "                if existing_data:\n",
    "                    # If the file already exists and adding new data exceeds 15 MB,\n",
    "                    # we keep the existing data and move to a new file\n",
    "                    file_index += 1\n",
    "                else:\n",
    "                    # If it's a new file and data alone exceeds 15 MB,\n",
    "                    # we need to split the data\n",
    "                    split_point = len(data) // 2\n",
    "                    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(data[:split_point], f, indent=4)\n",
    "                    data = data[split_point:]\n",
    "                    file_index += 1\n",
    "        \n",
    "        return file_index\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {csv_file}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "st_time = time.time()\n",
    "first_run_flag = True # Make this as False if you want to keep contents in output.json\n",
    "csv_folder = Path(r\"./resources\")\n",
    "last_file_index = 0\n",
    "\n",
    "for csv_path in csv_folder.glob(\"*\"):\n",
    "    if csv_path.suffix.lower() in ['.csv', '.xls', '.xlsx']:\n",
    "        url = input(f\"\\nPlease enter the URL for {csv_path.name} (press Enter to skip): \").strip()\n",
    "        if csv_path.suffix == '.csv':\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                last_file_index = csv_to_json(csv_path, url, \".csv\", first_run_flag)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "        elif csv_path.suffix in ['.xls', '.xlsx']:\n",
    "            result = excel_to_csv(csv_path)\n",
    "            if isinstance(result, tuple):\n",
    "                csv_path, conversion_flag = result\n",
    "                if csv_path and conversion_flag:\n",
    "                    last_file_index = csv_to_json(csv_path, url, conversion_flag, first_run_flag)\n",
    "                    os.remove(csv_path)\n",
    "            else:\n",
    "                print(result)  # Print the error message\n",
    "        first_run_flag = False\n",
    "        print(f\"\\nConversion of {csv_path.name} is successful\")\n",
    "\n",
    "ed_time = time.time()\n",
    "tot_time = ed_time - st_time\n",
    "print(\"\\n\\nSuccessfully converted csv data to json in {:.2f} seconds!\".format(tot_time))\n",
    "print(\"--------------------------------------------\")\n",
    "if last_file_index == 0:\n",
    "    print(\"Check: resources\\\\output.json for results!\")\n",
    "else:\n",
    "    print(f\"Check: resources\\\\output.json to resources\\\\output{last_file_index}.json for results!\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
